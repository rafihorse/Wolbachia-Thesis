{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## The First Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wednesday 07/01/2020\n",
    "\n",
    "The first step for making multiple sequence alignments (MSAs) for use with PAML to calculate dN/dS ratios is to identify the sequences that you would like to align. So, for each horizontally transferred gene, I need to select a number of homologous proteins for which I will estimate dN/dS ratios. The way I've done this is by BLASTing all of the HGT proteins against the nr database and taking the top 25 hits for each one, finding their CDS, and then using MACSE to align the CDS. I did this using small scripts I wrote that I have pasted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env bash\n",
    "#blast.sh\n",
    "\n",
    "#gunzip makeblastdb -in all_wolbachia_genomes.fna -out wolbachia_db -title wolbachia_db -dbtype nucl\n",
    "\n",
    "find . -name \"prot.part*.faa\" | parallel -j 4 \"blastp -query {} -out {}.out -db /Scratch/smnieves/databases/nr/nr -num_threads 8 -outfmt 6 -max_target_seqs 25\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is followed by ProteinGrouper.py to group the HGT proteins with all of their hits. Then this small script to retrieve all the CDS for each protein accession (I tested this and it should work, I'm **very** excited about it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilterBlast.py -dir ./ -percID 80 -len_align 100\n",
    "cat *_nolimit.out.filtered > hgtvnr_nolimit.out.filtered\n",
    "ProteinGrouper.py -file hgtvnr_nolimit.out.filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for creating accession lists for each homologous protein set\n",
    "for file in *.filtered\n",
    "do\n",
    "awk '{print $1}' $file | sort -u > ${file%.faa.out.filtered}_filtered.acc\n",
    "awk '{print $2}' $file | sort -u >> ${file%.faa.out.filtered}_filtered.acc\n",
    "done\n",
    "\n",
    "# for fetching fastas using protein blast accessions\n",
    "for file in *.acc\n",
    "do\n",
    "while read -r line\n",
    "do\n",
    "efetch -db protein -format fasta_cds_na -id $line >> ${file%.acc}.fna\n",
    "done < $file\n",
    "done\n",
    "\n",
    "# for fetching fastas using nucleotide blast accessions\n",
    "for file in protein_aln*.txt\n",
    "do\n",
    "outfile=${file%.txt}.fna\n",
    "faSomeRecords prot_cds.fna $file $outfile\n",
    "blastdbcmd -entry_batch $file -db /Scratch/smnieves/databases/nt/nt >> $outfile\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## The Better Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monday 08/31/2020 - Doing it over a new way\n",
    "So I probably made several mistakes above by making something more complicated than it needs to be. The *new* plan is the following:\n",
    "- [x] Download fastas for each protein\n",
    "- [x] Make a separate file for each fasta\n",
    "- [x] BLAST each fasta independently at nr\n",
    "- [ ] Retrieve fastas for all hits to nr\n",
    "- [ ] Make alignments\n",
    "- [ ] Trim alignments\n",
    "- [ ] Make trees\n",
    "- [ ] PAML (profit!)\n",
    "\n",
    "Below I outlined the method for obtaining the fastas for each protein, because NCBI Batch Entrez online is super unreliable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while read -r line\n",
    "do\n",
    " efetch -db protein -format fasta -id $line >> all_prot.faa \n",
    "done < ~/bin/arth_queries.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I make accession lists for all of the BLAST hits. After this I extract those accessions from the nr database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-8f9c27c75b98>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-8f9c27c75b98>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    FilterBlast.py -len_align 100 -dir ./\u001b[0m\n\u001b[1;37m                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# filter for hits greater than 100 aa alignments\n",
    "FilterBlast.py -len_align 100 -percID 70 -dir ./\n",
    "\n",
    "# create accession lists\n",
    "for file in *.filtered\n",
    "do\n",
    "awk '{print $1}' $file | sort -u > ${file%.faa.out.filtered}.acc\n",
    "awk '{print $2}' $file | sort -u >> ${file%.faa.out.filtered}.acc\n",
    "done\n",
    "\n",
    "# extract fastas\n",
    "for file in *.acc\n",
    "do\n",
    "echo $file\n",
    "outfile=${file%.acc}.fasta\n",
    "epost -db protein -input $file | elink -target nuccore -db protein -name protein_nuccore | efetch -format fasta_cds_na | seqkit seq -w 0 | grep -A 1 -Ff $file > $outfile\n",
    "sleep 1\n",
    "done\n",
    "\n",
    "# redo extraction on files that didn't work\n",
    "for file in $(find . -name \"*[0-9].fasta\" -size 0)\n",
    "do\n",
    "echo ${file%.fasta}.acc\n",
    "outfile=$file\n",
    "epost -db protein -input ${file%.fasta}.acc | elink -target nuccore -db protein -name protein_nuccore | efetch -format fasta_cds_na | seqkit seq -w 0 | grep -A 1 -Ff ${file%.fasta}.acc > $outfile\n",
    "sleep 3\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be useful for sanity checks to make sure the resulting multi-fasta file has the same number of sequences as accessions in the acc file. To do this, you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in *.acc\n",
    "do\n",
    "echo ${file%.acc}\n",
    "cat $file | wc -l\n",
    "cat ${file%.acc}.fasta | grep -c '^>'\n",
    "sleep 3\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wednesday 09/03/2020 - Correcting a potentially horrendous mistake\n",
    "So I realized that there was a problem with my **FilterBlast.py** program where it only kept hits with very low evalues. This was because I set my default evalue to 0, which is problematic for all the tests I did in the past.\n",
    "\n",
    "The first thing I'm going to do is go back and look at my *initial BLAST against my Wolbachia database*. I want to see if I excluded any proteins that I now need to do tests for. **Luckily it doesn't seem like I used FilterBlast.py in any other step.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wednesday 09/16/2020 - FINALLY Downloading CDS for **WP_** Proteins\n",
    "I just edited *The Better Way* above. It now **actually** is able to find the CDS for non-redundant protein accessions. What I have to do is fetch every fasta on the contig containing the WP accession, linearize them, and then grep for the sequence i want. It is a huge pain in the ass and pretty slow, but for now it works and I have not found another way.\n",
    "\n",
    "Below is me trying to implement the above in python for better error handling, but I shelved it. I realized I can actually do the whole process once for each file, instead of once per line per file, which should produce fewer errors. I'm editing the above code to reflect this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3.6\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "__author__ = 'Serafina Nieves'\n",
    "__email__ = 'smnieves@ucsc.edu'\n",
    "\n",
    "from Bio import Entrez\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "            description='Handle filtering arguments for the FilterBlast class.',\n",
    "            prefix_chars='-',\n",
    "            usage='prog [options] -option1[default] <input >output'\n",
    "        )\n",
    "\n",
    "parser.add_argument('-dir', action='store',\n",
    "                                 help='specify a valid path to directory containing protein accession files')\n",
    "parser.add_argument('-ext', action='store', default='.acc',\n",
    "                                 help='specify file type extension to correctly locate files')\n",
    "\n",
    "args = self.parser.parse_args()\n",
    "\n",
    "Entrez.email = \"smnieves@ucsc.edu\"\n",
    "\n",
    "def get_proteins(file, outfile):\n",
    "    Entrez.epost(file)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "directory_in_str = parser.args.dir\n",
    "    directory = os.fsencode(directory_in_str)\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        outfile = filename[:-4] + \".fasta\"\n",
    "        if filename.endswith(parser.args.ext):\n",
    "            fastas = get_proteins(directory_in_str + filename, directory_in_str + outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thursday 09/24/2020 - Finalizing CDS Retrieval\n",
    "The method outlined above for retrieving CDS seems to be working. It does take multiple passes to ensure that every query worked and downloaded a CDS. The good thing is that I think the whole query batch will fail, not just one accession out of the batch. This means I can search for files of 0 byte size to find the accession lists that need to be retried, instead of searching to see if individual accesssions downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Creating Alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuesday 09/22/2020 - Generating alignments\n",
    "The Entrez Direct process for downloading CDS takes a lot of time, so I am going to start generating the alignment files. The way I'll do this is by selecting the fasta files that are larger than 0 bytes, and then aligning those with MACSE. I will ask Russ tomorrow if I need to remove 100% identical sequences from these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find . -name \"*.fasta\" -size +0 | parallel -j 8 \"java -jar ~/bin/macse_v2.03.jar -prog alignSequences -seq {}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thursday 09/24/2020 - Trimming Alignments\n",
    "MACSE seems to work fairly well for the coding sequence multi-fasta inputs. However, some alignments show large gaps both on the ends and in the middles of sequences. **TrimAl** is an awesome tool to deal with this, as it appears I can trip gaps off the ends and in the middles. Most importantly though, I can specify that I don't want to keep gaps that appear in more than 30% of sequences. In other words, I can specify that there must be a nucleotide present at that position in at least 70% of sequences or it needs to go. I can additionally specify that I don't want to trim this gap if it means less than 60% of the original alignment will be kept in the new alignment. This seems like a super useful feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find . -name \"*NT.fasta\" | parallel -j 100 \"trimal -gt .85 -cons 60 -in {} -out {}.trimal\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions to ask Russ:\n",
    "- [ ] Do I just need to run ModelFinder once and then use the same model to generate every tree\n",
    "- [ ] For PAML should I use the branch model, site model, or branch-site model (I'm leaning towards branch-site)\n",
    "- [ ] Tell him that I put a high threshold for the number of sequences that had to have a nucleotide at a certain position because PAML deals poorly with gaps\n",
    "- [ ] How do I parse the PAML output files (programmatically or with my eyes lol)\n",
    "- [ ] What model (how many dN/dS ratios for codons) in the control file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Friday 09/25/2020 - Creating workable PHYLIP files\n",
    "TrimAl only supports phylip and phylip3.2. The problem is that PAML does not like the phylip3.2 format, but the original phylip format from TrimAl does not allow you to have longer sequence identifiers. This is problematic because it creates duplicate sequence identifiers in the alignment due to truncation. Below I will use Biopython to write my alignment files out to relaxed PHYLIP format. *Additionally this can create the control file for PAML.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import AlignIO\n",
    "\n",
    "alignment_fasta = AlignIO.read(\"all_prot.part-1156_NT.trimal\", \"fasta\")\n",
    "\n",
    "for record in alignment_fasta:\n",
    "    record.id = record.id[0:45].replace(\"|\", \"_\") + \"  \"\n",
    "    #print(record.id)\n",
    "\n",
    "unique_seq = set()\n",
    "for record in alignment_fasta:\n",
    "    if record.seq in unique_seq and record.id.contains(\"WP_\"):\n",
    "        alignment_fasta.pop(record.index())\n",
    "        \n",
    "with open(\"all_prot.part-1156_NT.phy\", \"w\") as outfile:\n",
    "    AlignIO.PhylipIO.SequentialPhylipWriter(outfile).write_alignment(unique_seq, id_width=47)\n",
    "    #AlignIO.write(alignment_fasta, outfile, \"phylip-relaxed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='all_prot.part-1156_NT.phy' mode='w' encoding='cp1252'>\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# create control files for PAML\n",
    "\n",
    "file_base = str(sys.argv[1])\n",
    "model = str(sys.argv[2])\n",
    "\n",
    "with open(\"/home/smnieves/bin/codeml.ctl\", \"r\") as ctl_template:\n",
    "    template = ctl_template.readlines()\n",
    "    \n",
    "template[0] = \"      seqfile = {0} * sequence data filename\\n\".format(file_base + \".phy\")\n",
    "template[1] = \"     treefile = {0}      * tree structure file name\\n\".format(file_base + \".phy.treefile\")\n",
    "template[18] = \"       model = {0}\\n\".format(model)\n",
    "\n",
    "with open(\"codeml.ctl\", 'w') as file:\n",
    "    file.writelines(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-950b4a3da1c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m cml.set_options(noisy=9, verbose=1, runmode=0, seqtype=1, CodonFreq=2,\n\u001b[1;32m---> 11\u001b[1;33m                \u001b[0mndata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maaDist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0micode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m                \u001b[0mMgene\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfix_kappa\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkappa\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfix_omega\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0momega\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                \u001b[0mfix_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMalpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncatG\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetSE\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRateAncestor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mod' is not defined"
     ]
    }
   ],
   "source": [
    "from Bio.Phylo.PAML import codeml\n",
    "\n",
    "wdir = str(sys.argv[1])\n",
    "seqfile = str(sys.argv[2])\n",
    "treefile = str(sys.argv[3])\n",
    "mod = str(sys.argv[4])\n",
    "\n",
    "cml = codeml.Codeml(working_dir=wdir, alignment=seqfile, tree=treefile, out_file=\"paml_results.out\")\n",
    "\n",
    "cml.set_options(noisy=9, verbose=1, runmode=0, seqtype=1, CodonFreq=2,\n",
    "               ndata=10, clock=0, aaDist=0, model=mod, NSsites=[0], icode=0,\n",
    "               Mgene=0, fix_kappa=0, kappa=2, fix_omega=0, omega=1,\n",
    "               fix_alpha=1, alpha=0., Malpha=0, ncatG=8, getSE=0, RateAncestor=1,\n",
    "               Small_Diff=.5e-6, cleandata=1, fix_blength=1, method=0)\n",
    "cml.print_options()\n",
    "cml.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "\n",
    "## Overview of AlntoPAML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Friday 10/23/2020 - Snakefiles\n",
    "So I decided instead of trying to string together a bunch of independent programs in a bash script, I was going to figure out snakemake. And I did! This allows me to:\n",
    "- Stop all jobs and restart them where they left off\n",
    "- Rerun the pipeline from any point\n",
    "- Rerun the pipeline and regenerate downstream files\n",
    "- Rerun the pipeline to generate missing dependencies\n",
    "- Keep all of my rules organized\n",
    "- Keep track of versions so I don't overwrite files by mistake\n",
    "- Easily assign resources so I don't have to manage them for each step in the pipeline\n",
    "\n",
    "\n",
    "So far this has worked really well. Below is the snakefile I used for the pipeline, named **AlntoPAML.snakefile**. Following it are any custom programs written for the purpose of this project. Programs that I did not write that are available on the command line do not appear below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDS = range(1, 1673)\n",
    "\n",
    "rule all:\n",
    "        input:\n",
    "                #expand(\"prot_{file}/paml_results_null.out\", file=IDS),\n",
    "                #expand(\"prot_{file}/paml_results_alt.out\", file=IDS),\n",
    "                expand(\"prot_{file}/CodemlModelCompare_prot_{file}.txt\", file=IDS)\n",
    "\n",
    "rule macse_alignseqs:\n",
    "        input:\n",
    "                \"all_prot.part-{file}.fasta\"\n",
    "        output:\n",
    "                NT = \"prot_{file}/all_prot.part-{file}_aligned_NT.fasta\",\n",
    "                AA = \"prot_{file}/all_prot.part-{file}_aligned_AA.fasta\"\n",
    "        shell:\n",
    "                \"java -jar ~/bin/macse_v2.03.jar -prog alignSequences -seq {input} \"\n",
    "                \"-out_NT {output.NT} -out_AA {output.AA}\"\n",
    "\n",
    "rule trim_alignments:\n",
    "        input:\n",
    "                \"prot_{file}/all_prot.part-{file}_aligned_NT.fasta\"\n",
    "        output:\n",
    "                \"prot_{file}/all_prot.part-{file}.trimal\"\n",
    "        shell:\n",
    "                \"trimal -gt .85 -resoverlap .75 -seqoverlap .85 -in {input} -out {output}\"\n",
    "\n",
    "rule macse_realignseqs:\n",
    "        input:\n",
    "                \"prot_{file}/all_prot.part-{file}.trimal\"\n",
    "        output:\n",
    "                NT = \"prot_{file}/all_prot.part-{file}_refined_NT.fasta\",\n",
    "                AA = \"prot_{file}/all_prot.part-{file}_refigned_AA.fasta\"\n",
    "        shell:\n",
    "                \"java -jar ~/bin/macse_v2.03.jar -prog refineAlignment -align {input} \"\n",
    "                \"-out_NT {output.NT} -out_AA {output.AA}\"\n",
    "\n",
    "rule macse_exportaln:\n",
    "        input:\n",
    "                \"prot_{file}/all_prot.part-{file}_refined_NT.fasta\"\n",
    "        output:\n",
    "                NT = \"prot_{file}/all_prot.part-{file}_noFSstop_NT.fasta\",\n",
    "                AA = \"prot_{file}/all_prot.part-{file}_noFSstop_AA.fasta\"\n",
    "        shell:\n",
    "                \"java -jar ~/bin/macse_v2.03.jar -prog exportAlignment -align {input} -codonForInternalStop NNN \"\n",
    "                \"-codonForFinalStop --- -codonForInternalFS --- -charForRemainingFS - \"\n",
    "                \"-out_NT {output.NT} -out_AA {output.AA}\"\n",
    "\n",
    "rule generate_phylip:\n",
    "        input:\n",
    "                \"prot_{file}/all_prot.part-{file}_noFSstop_NT.fasta\"\n",
    "        output:\n",
    "                \"prot_{file}/all_prot.part-{file}.phy\"\n",
    "        shell:\n",
    "                \"/home/smnieves/bin/WritePhylip.py {input} {output}\"\n",
    "\n",
    "rule generate_tree:\n",
    "        input:\n",
    "                \"prot_{file}/all_prot.part-{file}.phy\"\n",
    "        output:\n",
    "                \"prot_{file}/all_prot.part-{file}.phy.treefile\"\n",
    "        shell:\n",
    "                \"iqtree -s {input} -m GTR+G -redo -quiet\"\n",
    "\n",
    "rule annotate_tree:\n",
    "        input:\n",
    "                \"prot_{file}/all_prot.part-{file}.phy.treefile\"\n",
    "        output:\n",
    "                \"prot_{file}/all_prot.part-{file}.phy.foreground.treefile\"\n",
    "        shell:\n",
    "                \"AnnotateTree.py {input} {output}\"\n",
    "\n",
    "rule run_paml_null:\n",
    "        input:\n",
    "                seqfile=\"prot_{file}/all_prot.part-{file}.phy\",\n",
    "                treefile=\"prot_{file}/all_prot.part-{file}.phy.treefile\"\n",
    "        params:\n",
    "                wdir=\"prot_{file}\",\n",
    "                model=0\n",
    "        output:\n",
    "                \"prot_{file}/paml_results_null.out\"\n",
    "        shell:\n",
    "                \"PAML.py {params.wdir} {input.seqfile} {input.treefile} {params.model} {output}\"\n",
    "\n",
    "rule run_paml_alt:\n",
    "        input:\n",
    "                seqfile=\"prot_{file}/all_prot.part-{file}.phy\",\n",
    "                treefile=\"prot_{file}/all_prot.part-{file}.phy.foreground.treefile\"\n",
    "        params:\n",
    "                wdir=\"prot_{file}\",\n",
    "                model=2\n",
    "        output:\n",
    "                \"prot_{file}/paml_results_alt.out\"\n",
    "        shell:\n",
    "                \"PAML.py {params.wdir} {input.seqfile} {input.treefile} {params.model} {output}\"\n",
    "\n",
    "rule codeml_compare:\n",
    "        input:\n",
    "                null=\"prot_{file}/paml_results_null.out\",\n",
    "                alt=\"prot_{file}/paml_results_alt.out\"\n",
    "        output:\n",
    "                output=\"prot_{file}/CodemlModelCompare_prot_{file}.txt\"\n",
    "        params:\n",
    "                wdir=\"prot_{file}\"\n",
    "        shell:\n",
    "                \"CodemlCompare.py {params.wdir} {input.null} {input.alt} {output}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fc4227942d09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0malignment_fasta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAlignIO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fasta\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0moutput_alignment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAlign\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultipleSeqAlignment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\serafina nieves\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\Bio\\AlignIO\\__init__.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(handle, format, seq_count)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m         \u001b[0malignment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No records found in handle\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\serafina nieves\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\Bio\\AlignIO\\__init__.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(handle, format, seq_count)\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Need integer for seq_count (sequences per alignment)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mas_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Map the file format to a sequence iterator:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_FormatToIterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\serafina nieves\\appdata\\local\\programs\\python\\python38\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\serafina nieves\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\Bio\\File.py\u001b[0m in \u001b[0;36mas_handle\u001b[1;34m(handleish, mode, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandleish\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '-f'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3.6\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "__author__ = 'Serafina Nieves'\n",
    "__email__ = 'smnieves@ucsc.edu'\n",
    "\n",
    "'''WritePhylip.py writes a sequential Phylip file without sequence duplicates, limiting the sequence alignment to at most 50 sequences.'''\n",
    "\n",
    "from Bio import AlignIO\n",
    "from Bio import Align\n",
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "alignment_fasta = AlignIO.read(str(sys.argv[1]), \"fasta\")\n",
    "\n",
    "clean_alignment = Align.MultipleSeqAlignment([])\n",
    "\n",
    "arth_list = []\n",
    "with open(\"/home/smnieves/bin/arth_queries.txt\") as prot_list:\n",
    "    for line in prot_list:\n",
    "        arth_list.append(line.strip())\n",
    "\n",
    "for record in alignment_fasta:\n",
    "    record.id = record.id[0:45].replace(\"|\", \"_\") + \"  \"\n",
    "\n",
    "def deduplicate(alignment):\n",
    "    dedup_alignment = Align.MultipleSeqAlignment([])\n",
    "    unique_prot = set()\n",
    "    for record in alignment:\n",
    "        prot_name = record.id[record.id.index(\"cds_\") + 4: record.id.index(\".\", record.id.index(\"cds_\")) + 2]\n",
    "        if prot_name not in unique_prot:\n",
    "            unique_prot.add(prot_name)\n",
    "            dedup_alignment.append(record)\n",
    "    return(dedup_alignment)\n",
    "\n",
    "clean_alignment = deduplicate(alignment_fasta)\n",
    "\n",
    "if len(clean_alignment) > 50:\n",
    "    clean_alignment = random.sample(list(clean_alignment), 50)\n",
    "\n",
    "for record in alignment_fasta:\n",
    "    if any(acc in record.id for acc in arth_list):\n",
    "        clean_alignment.append(record)\n",
    "\n",
    "output_alignment = deduplicate(clean_alignment)\n",
    "\n",
    "\n",
    "with open(sys.argv[2], \"w\") as outfile:\n",
    "    AlignIO.PhylipIO.SequentialPhylipWriter(outfile).write_alignment(output_alignment, id_width=50)\n",
    "    #AlignIO.write(alignment_fasta, outfile, \"phylip-relaxed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3.6\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "__author__ = 'Serafina Nieves'\n",
    "__email__ = 'smnieves@ucsc.edu'\n",
    "\n",
    "import sys\n",
    "from Bio import Phylo\n",
    "\n",
    "'''AnnotateTree.py is a program that takes a list of accessions and annotates branches on the tree with those accessions as foreground branches.'''\n",
    "\n",
    "arth_list = []\n",
    "with open(\"/home/smnieves/bin/arth_queries.txt\") as prot_list:\n",
    "    for line in prot_list:\n",
    "        arth_list.append(line.strip())\n",
    "\n",
    "treefile = str(sys.argv[1])\n",
    "outfile = str(sys.argv[2])\n",
    "\n",
    "newick = Phylo.read(treefile, \"newick\")\n",
    "for clade in newick.find_clades(name=True):\n",
    "    for acc in arth_list:\n",
    "        try:\n",
    "            if acc in clade.name:\n",
    "                clade.name = clade.name + \"#1\"\n",
    "        except(TypeError):\n",
    "             continue\n",
    "# for clade in newick.find_clades():\n",
    "    # print(clade.name)\n",
    "\n",
    "Phylo.write(newick, outfile, format=\"newick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3.6\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "__author__ = 'Serafina Nieves'\n",
    "__email__ = 'smnieves@ucsc.edu'\n",
    "\n",
    "from Bio.Phylo.PAML import codeml\n",
    "import sys\n",
    "\n",
    "''' PAML.py is a program that allows fast customization of PAML parameters and reproducible control files. It can specify the parameters and\n",
    "the working directory to keep files together. The parameters can be written to a control file and the output file can be parsed (not currently implemented).'''\n",
    "\n",
    "wdir = str(sys.argv[1])\n",
    "seqfile = str(sys.argv[2])\n",
    "treefile = str(sys.argv[3])\n",
    "mod = str(sys.argv[4])\n",
    "outfile= str(sys.argv[5])\n",
    "\n",
    "cml = codeml.Codeml(working_dir=wdir, alignment=seqfile, tree=treefile, out_file=outfile)\n",
    "\n",
    "cml.set_options(noisy=9, verbose=1, runmode=0, seqtype=1, CodonFreq=2,\n",
    "               ndata=0, clock=0, aaDist=0, model=mod, NSsites=[0], icode=0,\n",
    "               Mgene=0, fix_kappa=0, kappa=2, fix_omega=0, omega=1,\n",
    "               fix_alpha=1, alpha=0., Malpha=0, ncatG=8, getSE=0, RateAncestor=0,\n",
    "               Small_Diff=.5e-6, cleandata=1, fix_blength=1, method=0)\n",
    "\n",
    "cml.print_options()\n",
    "cml.run(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3.6\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "__author__ = 'Serafina Nieves'\n",
    "__email__ = 'smnieves@ucsc.edu'\n",
    "\n",
    "from Bio.Phylo.PAML import codeml\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.stats.distributions import chi2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "''' CodemlCompare.py compares the null and 2 ratio model of codeml to determine which model fits the data better. '''\n",
    "\n",
    "# set parameters for the program (flags)\n",
    "x_points = []\n",
    "y_points = []\n",
    "string_towrite = []\n",
    "\n",
    "\n",
    "num_list = range(1, 1673)\n",
    "for num in num_list:\n",
    "    try:\n",
    "        wdir = \"prot_\" + str(num)\n",
    "        infile_null = wdir + \"/paml_results_null.out\"\n",
    "        infile_alt = wdir + \"/paml_results_alt.out\"\n",
    "        outfile = \"/Scratch/smnieves/alignments/CodemlModelCompare.txt\"\n",
    "        outfile2 = wdir + \"/CodemlCompare_prot\" + str(num) + \".txt\"\n",
    "\n",
    "        # read in data and parse it for relevant values\n",
    "        print(infile_null)\n",
    "        results_null = codeml.read(infile_null)\n",
    "        results_alt = codeml.read(infile_alt)\n",
    "\n",
    "        lnL_null = results_null.get(\"NSsites\").get(0).get(\"lnL\")\n",
    "        lnL_alt = results_alt.get(\"NSsites\").get(0).get(\"lnL\")\n",
    "\n",
    "        likelihood_ratio = -2 * (lnL_null - lnL_alt)\n",
    "        p_value = chi2.sf(likelihood_ratio, 1)\n",
    "\n",
    "        arth_omega = results_alt.get(\"NSsites\").get(0).get(\"parameters\").get(\"omega\")[1]\n",
    "        background_omega = results_alt.get(\"NSsites\").get(0).get(\"parameters\").get(\"omega\")[0]\n",
    "\n",
    "        # save point data and plot points\n",
    "        print(background_omega, arth_omega)\n",
    "        x_points.append(background_omega)\n",
    "        y_points.append(arth_omega)\n",
    "\n",
    "        # create formatted strings\n",
    "        genes_interest = []\n",
    "        arth_list = open(\"/home/smnieves/bin/arthqueries.txt\").readlines()\n",
    "        if p_value <= 0.05:\n",
    "            dir_num = wdir[5:]\n",
    "            acc_file = \"all_prot.part-\" + dir_num + \".acc\"\n",
    "            for line in open(acc_file):\n",
    "                if line in arth_list:\n",
    "                    genes_interest.append(line.strip())\n",
    "\n",
    "        string_towrite.append(\"directory: {0} \\t likelihood ratio: {1} \\t p-value: {2} \\t arth_genes: {3} \\n\".format(wdir, likelihood_ratio, p_value, \",\".join(genes_interest)))\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "        \n",
    "# change dimensions of the entire figure\n",
    "figureHeight = 5\n",
    "figureWidth = 5\n",
    "\n",
    "plt.figure(figsize=(figureWidth, figureHeight))\n",
    "\n",
    "mainPanelWidth = 4\n",
    "mainPanelHeight = 4\n",
    "\n",
    "mainPanel = plt.axes([0.1, 0.1, mainPanelWidth / figureWidth, mainPanelHeight / figureHeight])\n",
    "\n",
    "mainPanel.scatter(x_points, y_points)\n",
    "plt.savefig('test_fig.png', dpi=300)\n",
    "\n",
    "# write formatted strings to files\n",
    "with open(outfile, 'w') as file:\n",
    "    for string in string_towrite:\n",
    "        file.write(string)\n",
    "\n",
    "with open(outfile2, 'w') as file2:\n",
    "    for string in string_towrite:\n",
    "        file2.write(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Characteristics of Transferred Genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saturday 02/20/2021 - Insertion Locations\n",
    "This is the code I used to do a BLAST to find the insertion locations of the transferred genes. I added qlen and slen so that I could find the subject coverage of the alignment, keeping only subject coverages less than 10% of an arthropod genes length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find . -name \"*.faa\" | parallel -j 8 \"blastp -query {} -out {}.out -taxidlist arthropoda_taxids.txt -db /Scratch/smnieves/databases/nr/nr -outfmt '6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore qlen slen staxids sscinames scomnames sskingdoms' -num_threads 4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These BLAST out files are then filtered with awk, looking for alignments of the query and subject that constitute less than 10% of the subject's overall length, and that have 80% sequence identity in the aligned region. These files are named with the scheme: file.faa.out_10aln_80id.filtered, where the 10aln specifies that less than 10% of the subject sequence must be aligned and that greater than 80% sequence identity must be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in *.out; do cat $file | awk '$4/$13 < .1 && $3 > 90' > ${file}_10aln_90id.filtered; done\n",
    "find . -name '*.filtered' -empty -type f -delete\n",
    "ls *filtered"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
